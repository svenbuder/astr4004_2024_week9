{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5366df",
   "metadata": {},
   "source": [
    "# ANU ASTR4004 2024 - Week 8 (24+26 September 2024)\n",
    "\n",
    "Author: Sven Buder (sven.buder@anu.edu.au)\n",
    "\n",
    "Based on the tutorial by Yuan-Sen Ting from ASTR4004 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a203675",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Our-data:-APOGEE-DR17-and-GALAH-DR4\" data-toc-modified-id=\"Our-data:-APOGEE-DR17-and-GALAH-DR4-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Our data: APOGEE DR17 and GALAH DR4</a></span></li><li><span><a href=\"#k-means-with-APOGEE-DR17\" data-toc-modified-id=\"k-means-with-APOGEE-DR17-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>k-means with APOGEE DR17</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initializing-K-means-Clustering-Parameters\" data-toc-modified-id=\"Initializing-K-means-Clustering-Parameters-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Initializing K-means Clustering Parameters</a></span></li><li><span><a href=\"#Visualizing-the-Data-and-Cluster-Centroids\" data-toc-modified-id=\"Visualizing-the-Data-and-Cluster-Centroids-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Visualizing the Data and Cluster Centroids</a></span></li><li><span><a href=\"#Optimizing-the-K-means-Clustering\" data-toc-modified-id=\"Optimizing-the-K-means-Clustering-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Optimizing the K-means Clustering</a></span></li><li><span><a href=\"#Evaluating-the-K-means-Model\" data-toc-modified-id=\"Evaluating-the-K-means-Model-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Evaluating the K-means Model</a></span></li><li><span><a href=\"#Monitoring-Convergence-in-K-means\" data-toc-modified-id=\"Monitoring-Convergence-in-K-means-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Monitoring Convergence in K-means</a></span></li><li><span><a href=\"#Visualising-EM\" data-toc-modified-id=\"Visualising-EM-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Visualising EM</a></span></li><li><span><a href=\"#Why-May-K-means-Not-Work-Well?\" data-toc-modified-id=\"Why-May-K-means-Not-Work-Well?-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Why May K-means Not Work Well?</a></span></li></ul></li><li><span><a href=\"#Gaussian-Mixture-Models-with-APOGEE-DR17\" data-toc-modified-id=\"Gaussian-Mixture-Models-with-APOGEE-DR17-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Gaussian Mixture Models with APOGEE DR17</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-and-Log-Likelihood-function\" data-toc-modified-id=\"Linear-and-Log-Likelihood-function-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Linear and Log-Likelihood function</a></span></li><li><span><a href=\"#Maximizing-Log-Likelihood-Through-EM-Algorithm\" data-toc-modified-id=\"Maximizing-Log-Likelihood-Through-EM-Algorithm-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Maximizing Log-Likelihood Through EM Algorithm</a></span></li><li><span><a href=\"#Initializing-Gaussian-Mixture-Model-Parameters\" data-toc-modified-id=\"Initializing-Gaussian-Mixture-Model-Parameters-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Initializing Gaussian Mixture Model Parameters</a></span></li><li><span><a href=\"#Visualizing-the-Data-and-Gaussian-Components\" data-toc-modified-id=\"Visualizing-the-Data-and-Gaussian-Components-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Visualizing the Data and Gaussian Components</a></span></li><li><span><a href=\"#Optimizing-the-Gaussian-Mixture-Model-with-EM\" data-toc-modified-id=\"Optimizing-the-Gaussian-Mixture-Model-with-EM-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Optimizing the Gaussian Mixture Model with EM</a></span></li><li><span><a href=\"#Evaluating-the-Model\" data-toc-modified-id=\"Evaluating-the-Model-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Evaluating the Model</a></span></li><li><span><a href=\"#Off-the-shelf-GMM\" data-toc-modified-id=\"Off-the-shelf-GMM-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Off-the-shelf GMM</a></span></li><li><span><a href=\"#How-many-components-do-we-need?\" data-toc-modified-id=\"How-many-components-do-we-need?-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>How many components do we need?</a></span></li></ul></li><li><span><a href=\"#Interactive:-GALAH-DR4-instead-of-APOGEE-DR17\" data-toc-modified-id=\"Interactive:-GALAH-DR4-instead-of-APOGEE-DR17-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Interactive: GALAH DR4 instead of APOGEE DR17</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ccb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "\n",
    "# We will use these later for the Gaussian Mixture Models\n",
    "from scipy.stats import multivariate_normal as mvnorm\n",
    "from scipy.stats import chi2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Make the size and fonts larger for this presentation\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['legend.fontsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95c25db",
   "metadata": {},
   "source": [
    "## Our data: APOGEE DR17 and GALAH DR4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897143b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    np.random.seed(4)\n",
    "    \n",
    "    # Read in GALAH DR4\n",
    "    galah_dr4 = Table.read('data/galah_dr4_allstar_240705.fits')\n",
    "\n",
    "    # Apply quality cuts\n",
    "    select_galah = (\n",
    "        (galah_dr4['flag_sp'] == 0) & (galah_dr4['flag_mg_fe'] == 0) & \n",
    "        (galah_dr4['fe_h'] > -1.0) & (galah_dr4['fe_h'] < 0.75) &\n",
    "        (galah_dr4['mg_fe'] > -0.35) & (galah_dr4['mg_fe'] < 0.65)\n",
    "    )\n",
    "    galah_dr4 = galah_dr4[['fe_h','mg_fe']][select_galah]\n",
    "\n",
    "    # Only for this tutorial: select only 50,000 randomly drawn entries\n",
    "    random_indices = np.random.choice(len(galah_dr4['fe_h']), size=50000, replace=False)\n",
    "    galah_dr4 = galah_dr4[random_indices]\n",
    "\n",
    "    # Save data\n",
    "    galah_dr4.write('data/galah_dr4_feh_mgfe.fits', overwrite=True)\n",
    "\n",
    "except:\n",
    "    galah_dr4 = Table.read('data/galah_dr4_feh_mgfe.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db289c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    np.random.seed(17)\n",
    "\n",
    "    # Read in APOGEE DR17\n",
    "    apogee_dr17 = Table.read('data/allStar-dr17-synspec_rev1.fits',1)\n",
    "\n",
    "    # Apply quality cuts\n",
    "    select_apogee = (\n",
    "        # No major flag\n",
    "        (apogee_dr17['ASPCAPFLAG'] == 0) &\n",
    "        # No flag for abundances\n",
    "        (apogee_dr17['FE_H_FLAG'] == 0) & (apogee_dr17['MG_FE_FLAG'] == 0) &\n",
    "        # Select disk stars\n",
    "        (apogee_dr17['FE_H'] > -1.0) & (apogee_dr17['FE_H'] < 0.75) &\n",
    "        (apogee_dr17['MG_FE'] > -0.35) & (apogee_dr17['MG_FE'] < 0.65) &\n",
    "        (apogee_dr17['MG_FE'] + apogee_dr17['FE_H'] > -0.75) &\n",
    "        # Select giants\n",
    "        (apogee_dr17['TEFF'] < 5500) & (apogee_dr17['LOGG'] < 3.5)\n",
    "    )\n",
    "    apogee_dr17 = apogee_dr17[['FE_H','MG_FE']][select_apogee]\n",
    "\n",
    "    # Only for this tutorial: select only 50,000 randomly drawn entries\n",
    "    random_indices = np.random.choice(len(apogee_dr17['FE_H']), size=50000, replace=False)\n",
    "    apogee_dr17 = apogee_dr17[random_indices]\n",
    "\n",
    "    # Save data\n",
    "    apogee_dr17.write('data/apogee_dr17_feh_mgfe.fits', overwrite=True)\n",
    "\n",
    "except:\n",
    "    apogee_dr17 = Table.read('data/apogee_dr17_feh_mgfe.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7c554",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Let's plot the [Fe/H] vs. [Mg/Fe] diagrams for APOGEE DR17 and GALAH DR4\n",
    "\n",
    "f, gs = plt.subplots(1,2,figsize=(10,3.5),sharex=True,sharey=True)\n",
    "\n",
    "ax = gs[0]\n",
    "ax.set_xlabel('[Fe/H]')\n",
    "ax.set_ylabel('[Mg/Fe')\n",
    "ax.set_title('APOGEE DR17')\n",
    "h = ax.hist2d(\n",
    "    apogee_dr17['FE_H'],\n",
    "    apogee_dr17['MG_FE'],\n",
    "    bins = 100,\n",
    "    norm = LogNorm()\n",
    ")\n",
    "cbar = plt.colorbar(h[-1], ax=ax, label = 'Nr.')\n",
    "\n",
    "ax = gs[1]\n",
    "ax.set_xlabel('[Fe/H]')\n",
    "ax.set_title('GALAH DR4')\n",
    "h = ax.hist2d(\n",
    "    galah_dr4['fe_h'],\n",
    "    galah_dr4['mg_fe'],\n",
    "    bins = 100,\n",
    "    norm = LogNorm()\n",
    ")\n",
    "cbar = plt.colorbar(h[-1], ax=ax, label = 'Nr.')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a798bd1",
   "metadata": {},
   "source": [
    "## k-means with APOGEE DR17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this tutorial, let's use variables rather than keywords\n",
    "feh  = np.array(apogee_dr17['FE_H'])\n",
    "mgfe = np.array(apogee_dr17['MG_FE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noramlize data\n",
    "feh = (feh - np.mean(feh)) / np.std(feh)\n",
    "mgfe = (mgfe - np.mean(mgfe)) / np.std(mgfe)\n",
    "\n",
    "# prepare data for sklearn\n",
    "data = np.vstack([feh, mgfe]).T\n",
    "data_labels = [\"[Fe/H]\", \"[Mg/Fe]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11957f",
   "metadata": {},
   "source": [
    "### Initializing K-means Clustering Parameters\n",
    "\n",
    "Before running the K-means algorithm on our dataset, it's important to set initial values for the model's centroids. These initializations guide the iterative process of the K-means algorithm. For our problem, we'll use a two-cluster K-means model to capture the bimodal distribution of alpha-enriched and alpha-normal stars.\n",
    "\n",
    "We will initialize the centroids of our two clusters as follows:\n",
    "\n",
    "- For the first cluster ($ C_1 $):\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    -1 \\\\\n",
    "    +1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- For the second cluster ($ C_2 $):\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    +1 \\\\\n",
    "    -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "By initializing these parameters, we can now proceed with the K-means algorithm to cluster our dataset based on elemental abundances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7325732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means initial centroids\n",
    "C_0 = np.array([[-1.0, 1.0], [1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15fa3c",
   "metadata": {},
   "source": [
    "### Visualizing the Data and Cluster Centroids\n",
    "\n",
    "To better understand the iterative process of the K-means algorithm and to visually inspect how well the centroids represent different clusters in the dataset, we plot both the data points and the centroids. Optionally, we can also visualize the \"redness\" of each data point, which could represent its closeness to a specific centroid or its likelihood of belonging to a particular cluster.\n",
    "\n",
    "**Helper Functions for Plotting: plot_data_and_centroids Function**\n",
    "\n",
    "The `plot_data_and_centroids` function serves the primary role of plotting both the data and the centroids on the same chart. Here are the parameters:\n",
    "\n",
    "- `data`: The actual data points you wish to plot.\n",
    "- `centroids`: The positions of the centroids.\n",
    "- `redness`: Optional parameter to color data points based on some metric, which could be their distance to the nearest centroid or some other relevant measure.\n",
    "\n",
    "This function uses an optional 'redness' parameter to color the data points. When the 'redness' values are provided, each point will be colored on a blue-to-red scale based on this value. This visual cue can help us better understand the cluster assignment at each step of the K-means iteration.\n",
    "\n",
    "**Optional: Custom Color Map**\n",
    "\n",
    "A custom color map (`br_cmap`) is defined using Matplotlib's `LinearSegmentedColormap`. This color map is used when coloring data points based on their 'redness' value. The color map transitions from blue to red, allowing for a clear visualization of cluster memberships or distances to centroids.\n",
    "\n",
    "These helper functions enable us to visually assess how well the K-means algorithm is performing at each iteration and offer insights into the clustering process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63734dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_and_centroids(data, centroids, redness=None):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    \n",
    "    br_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"seismic\",[\"b\",\"r\"])\n",
    "    \n",
    "    # Plotting the data points\n",
    "    if redness is not None:\n",
    "        assert len(redness) == data.shape[0]\n",
    "        assert all(_ >= 0 and _ <= 1 for _ in redness)\n",
    "        c = redness\n",
    "    else:\n",
    "        c = 'grey'\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=c, s=0.1, alpha=0.3, linewidths=2, cmap=br_cmap, label=\"Data points\")\n",
    "    \n",
    "    # Plotting the centroids with different colors\n",
    "    centroid_colors = ['b', 'r']\n",
    "    for i in range(centroids.shape[0]):\n",
    "        plt.scatter(centroids[i, 0], centroids[i, 1], c=centroid_colors[i], marker='x', s=100, label=f\"Centroid {i+1}\")\n",
    "    \n",
    "    plt.xlabel(\"[Fe/H]\")\n",
    "    plt.ylabel(\"[Mg/Fe]\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming data and initial centroids C_0 are defined earlier, and that 'redness' values are calculated somehow\n",
    "plot_data_and_centroids(data, C_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d6f46",
   "metadata": {},
   "source": [
    "### Optimizing the K-means Clustering\n",
    "\n",
    "As discussed in lectures, the K-means algorithm can be optimized through an iterative approach. This involves two steps: the assignment step, and the update step.\n",
    "\n",
    "**Function Signatures**\n",
    "\n",
    "The primary functions involved in K-means are:\n",
    "\n",
    "- For the E-step or Assignment step,\n",
    "```python\n",
    "def assignment_step(X, centroids):\n",
    "```\n",
    "\n",
    "- For the M-step or the Update step,\n",
    "```python\n",
    "def update_step(X, labels):\n",
    "```\n",
    "\n",
    "**Tips**\n",
    "\n",
    "You can make use of the `scipy.spatial.distance` library to compute Euclidean distances between points:\n",
    "\n",
    "```python\n",
    "from scipy.spatial import distance\n",
    "```\n",
    "\n",
    "You can use `distance.euclidean(a, b)` to compute the distance between points `a` and `b`.\n",
    "\n",
    "In this K-means implementation:\n",
    "\n",
    "1. `assignment_step`: Assigns each data point to the nearest centroid, effectively partitioning the data into clusters.\n",
    "2. `update_step`: Calculates the new centroids by taking the mean of all the data points in each cluster.\n",
    "\n",
    "Unlike GMM, K-means doesn't work with covariance matrices, Gaussian components, or weighted probabilities. It's a simpler algorithm in that regard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36309771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "def assignment_step(X, centroids):\n",
    "    \"\"\"\n",
    "    Performs the E-step of the K-means algorithm.\n",
    "    \"\"\"\n",
    "    labels = np.argmin([distance.cdist(X, c.reshape(1, -1), 'euclidean') for c in centroids], axis=0)\n",
    "    return labels.flatten()\n",
    "\n",
    "def update_step(X, labels, K):\n",
    "    \"\"\"\n",
    "    Performs the M-step of the K-means algorithm.\n",
    "    \"\"\"\n",
    "    new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(K)])\n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e7901",
   "metadata": {},
   "source": [
    "### Evaluating the K-means Model\n",
    "\n",
    "To evaluate the performance of the K-means model, you can calculate its inertia.\n",
    "\n",
    "The function computes the inertia by summing up the squared Euclidean distances between each data point and the centroid of its cluster. The resulting value will be a single scalar that measures how well the centroids fit the data $ \\mathbf{X} $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eddd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "def calculate_inertia(X, labels, centroids):\n",
    "    inertia = 0\n",
    "    for i in range(len(centroids)):\n",
    "        cluster_points = X[labels == i]\n",
    "        inertia += np.sum([distance.euclidean(p, centroids[i]) ** 2 for p in cluster_points])\n",
    "    return inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4fdb8",
   "metadata": {},
   "source": [
    "### Monitoring Convergence in K-means\n",
    "\n",
    "To check if the K-means algorithm is converging, one can track the model's inertia against the number of iterations. The idea is to run K-means for a fixed number of iterations and compute the inertia at each step. This information can then be visualized to assess the algorithm's convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2598af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `data` is your input data\n",
    "# Initialize centroids\n",
    "initial_centroids = C_0  \n",
    "\n",
    "# Number of clusters\n",
    "K = 2\n",
    "\n",
    "# Number of iterations\n",
    "iterations = 100\n",
    "\n",
    "# Array to hold the inertia values\n",
    "inertia_values = np.zeros(iterations)\n",
    "\n",
    "# Initialize centroids to some starting values (C_0)\n",
    "centroids = initial_centroids\n",
    "\n",
    "# Run the K-means algorithm\n",
    "for i in range(iterations):\n",
    "    # Perform the Assignment step to update labels\n",
    "    labels = assignment_step(data, centroids)\n",
    "    \n",
    "    # Compute the inertia of the current model\n",
    "    inertia_values[i] = calculate_inertia(data, labels, centroids)\n",
    "    \n",
    "    # Update centroids for the next iteration\n",
    "    centroids = update_step(data, labels, K)\n",
    "\n",
    "# Plotting the inertia values from iteration 0\n",
    "plt.title(\"Inertia Values from Iteration 0\")\n",
    "plt.xlabel(\"Number of Updates\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.plot(inertia_values)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the inertia values starting from iteration 1\n",
    "plt.title(\"Inertia Values from Iteration 1\")\n",
    "plt.xlabel(\"Number of Updates\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.plot(inertia_values[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbdae22",
   "metadata": {},
   "source": [
    "### Visualising EM\n",
    "\n",
    "Use the function `plot_data_and_centroids` to visualize the data points and your current cluster centroids at different stages of the K-means algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43077a66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize centroids to some starting values (C_0)\n",
    "centroids = C_0 \n",
    "\n",
    "# Number of clusters\n",
    "K = 2\n",
    "\n",
    "# Number of iterations for the K-means algorithm\n",
    "iterations = 50\n",
    "\n",
    "# Array to keep track of the inertia values\n",
    "inertia_values = np.zeros(iterations)\n",
    "\n",
    "# Run the K-means algorithm\n",
    "for i in range(iterations):\n",
    "    \n",
    "    # Perform the Assignment step to update labels\n",
    "    labels = assignment_step(data, centroids)\n",
    "    \n",
    "    # Compute the inertia of the current model\n",
    "    inertia_values[i] = calculate_inertia(data, labels, centroids)\n",
    "    \n",
    "    # Visualize the model every 5 iterations\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Iteration {i}: Current Inertia = {inertia_values[i]}\")\n",
    "        \n",
    "        # Show the current state of the model\n",
    "        plot_data_and_centroids(data, centroids, redness=labels)\n",
    "        \n",
    "    # Perform the Update step to update centroids\n",
    "    centroids = update_step(data, labels, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ad4f0",
   "metadata": {},
   "source": [
    "### Why May K-means Not Work Well?\n",
    "\n",
    "In our analysis, we observed that the K-means clustering algorithm did not perform well with our elemental abundances data that forms a double moon shape. The reason lies in the assumptions behind the K-means algorithm, particularly its use of Euclidean distance to partition data points into clusters. K-means tries to minimize the variance within each cluster, which is equivalent to minimizing the Euclidean distance from each data point to its cluster's centroid. This works well for clusters that are spherical and equally sized, but not for data with more complex shapes or varying densities.\n",
    "\n",
    "**Limitations of K-means:**\n",
    "\n",
    "1. **Spherical Assumption**: K-means assumes that the clusters are spherical and equally sized, which doesn't hold true for double moon-shaped data.\n",
    "  \n",
    "2. **Euclidean Distance**: The algorithm uses Euclidean distance to allocate points to the nearest cluster. For more complex shapes like double moons, a different distance measure or model might be more appropriate.\n",
    "  \n",
    "3. **Lack of Flexibility**: K-means does not have the flexibility to account for different shapes or orientations of clusters.\n",
    "\n",
    "To overcome these limitations, we turn to Gaussian Mixture Models (GMMs), a more flexible approach that can model elliptical clusters and is capable of handling different cluster shapes and orientations. Unlike K-means, GMMs do not assume equal-sized or spherical clusters, and they provide a probabilistic framework to capture uncertainty, making them a more suitable option for clustering our double moon-shaped data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7bed8",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models with APOGEE DR17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229967ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this tutorial, let's again use variables rather than keywords\n",
    "feh  = np.array(apogee_dr17['FE_H'])\n",
    "mgfe = np.array(apogee_dr17['MG_FE'])\n",
    "\n",
    "data = np.vstack([feh, mgfe]).T\n",
    "data_labels = [\"[Fe/H]\", \"[Mg/Fe]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4fd219",
   "metadata": {},
   "source": [
    "Gaussian Mixture Models (GMMs) offer a robust approach to model complex data distributions. When given a dataset $\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\}$ with each $\\mathbf{x}_i \\in \\mathbb{R}^D$, GMMs postulate that this data is generated from a sum of $K$ different Gaussian distributions.\n",
    "\n",
    "We want to find $K$ components, each with a mixture weight $\\pi_k$ and individual Gaussian distributions $\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k )$ with means $\\mu_k$ and covariances $\\Sigma_k$. The mixture weights must satisfy the constraint:\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K \\pi_k = 1\n",
    "$$\n",
    "\n",
    "The goal is to find $\\pi$, $\\mu$, and $\\Sigma$ that maximize this function. However, this is easier said than done due to the complex inter-dependencies between these parameters.\n",
    "\n",
    "### Linear and Log-Likelihood function\n",
    "\n",
    "Imagine drawing a random number $p$ uniformly from the interval $[0,1)$. Based on the value of $p$, we determine which Gaussian distribution to sample from. For example, if $p$ lies in the range $[\\sum_{i=1}^{k-1} \\pi_i, \\sum_{i=1}^{k} \\pi_k)$, we sample $\\mathbf{x}$ from $\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k )$.\n",
    "\n",
    "Mathematically, we can then describe the probability distribution $p$:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) := \\sum_{k=1}^K \\pi_k \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k )\n",
    "$$\n",
    "\n",
    "The log-likelihood function for GMMs is expressed as:\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{x} \\mid \\pi, \\mu, \\Sigma) \n",
    "= \\sum_{n=1}^N \\log \\left\\{ \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x} \\mid \\mu_k, \\Sigma_k) \\right\\}\n",
    "$$\n",
    "\n",
    "### Maximizing Log-Likelihood Through EM Algorithm\n",
    "\n",
    "**Responsibilities and Effective Number of Points**\n",
    "\n",
    "We introduce a variable called the 'responsibility', denoted by $\\gamma_{nk}$, which essentially quantifies the likelihood that the $n^{th}$ data point is generated by the $k^{th}$ Gaussian. It is computed as:\n",
    "\n",
    "$$\n",
    "\\gamma_{nk} = \\frac{\\pi_{k} \\mathcal{N}( \\mathbf{x}_n \\mid \\mu_k, \\Sigma_k ) }\n",
    "    { \\sum_{k=1}^K \\pi_{k} \\mathcal{N}( \\mathbf{x}_n \\mid \\mu_k, \\Sigma_k)}\n",
    "$$\n",
    "\n",
    "We then define $N_k$, the effective number of points for the $k^{th}$ Gaussian, as:\n",
    "\n",
    "$$\n",
    "N_k := \\sum_{n=1}^N \\gamma_{nk}\n",
    "$$\n",
    "\n",
    "**EM Algorithm Steps**\n",
    "\n",
    "The Expectation-Maximization (EM) algorithm proceeds in a series of steps to iteratively update the model parameters and maximize the log-likelihood.\n",
    "\n",
    "- **Step 1 (Initialization)**: Choose initial values for $\\mu_k, \\Sigma_k, \\pi_k$, and calculate the initial log-likelihood.\n",
    "\n",
    "- **Step 2 (Expectation)**: Compute the responsibilities using the current parameters.\n",
    "\n",
    "- **Step 3 (Maximization)**: Update the parameters based on the newly computed responsibilities:\n",
    "\n",
    "\\begin{align*}\n",
    "N_k^\\text{new} & := \\sum_{n=1}^N \\gamma_{nk} \\\\\n",
    "\\pi_k^{\\text{new}} & := \\frac{N_k^\\text{new}}{N} \\\\\n",
    "\\mu_k^\\text{new} & := \\frac{1}{N_k^\\text{new}} \\sum_{n=1}^N \\gamma_{nk} \\mathbf{x}_n \\\\\n",
    "\\Sigma_k^\\text{new} & := \\frac{1}{N_k^\\text{new}} \\sum_{n=1}^N \\gamma_{nk} (\\mathbf{x}_n - \\mu_k^\\text{new}) (\\mathbf{x}_n - \\mu_k^\\text{new})^T\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "- **Step 4 (Evaluate)**: Calculate the new log-likelihood. If it hasn't converged, return to Step 2.\n",
    "\n",
    "By iteratively applying these steps, the EM algorithm arrives at an optimized set of parameters that maximize the log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a2af8",
   "metadata": {},
   "source": [
    "### Initializing Gaussian Mixture Model Parameters\n",
    "\n",
    "Before running the Gaussian Mixture Model (GMM) on our dataset, it's crucial to set initial values for the model's parameters. These initializations guide the iterative process of the Expectation-Maximization (EM) algorithm, which we will be using to optimize the model. For our problem, we'll use a two-component Gaussian Mixture Model to capture the bimodal distribution of alpha-enriched and alpha-normal stars.\n",
    "\n",
    "**Initial Means: $ \\mu_0 $**\n",
    "\n",
    "We will initialize the means of our two Gaussian components as follows:\n",
    "\n",
    "- For the first Gaussian ($ \\mu_1 $):\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    -1 \\\\\n",
    "    +1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- For the second Gaussian ($ \\mu_2 $):\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    +1 \\\\\n",
    "    -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Initial Covariance Matrices: $ \\Sigma_0 $**\n",
    "\n",
    "Both covariance matrices will be initialized as identity matrices. This assumes, initially, that the features are uncorrelated and have a unit variance.\n",
    "\n",
    "**Initial Mixture Weights: $ \\pi_0 $**\n",
    "\n",
    "Initially, we will assume that both Gaussian components are equally likely to generate any given data point. Thus, the initial mixture weights will be equal; specifically, each will be set to 0.5.\n",
    "\n",
    "**Summary of Initial Parameters**\n",
    "\n",
    "- `mu_0`: A $2 \\times 2$ matrix containing the initial means of the Gaussians.\n",
    "  - $\\mu_0 = \\left[ \\begin{array}{cc} -1 & 1 \\\\ 1 & -1 \\end{array} \\right]$\n",
    "\n",
    "- `Sigma_0`: A $2 \\times 2 \\times 2$ 3-tensor containing the initial covariance matrices.\n",
    "  - $\\Sigma_0 = \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array} \\right] \\times 2$\n",
    "\n",
    "- `pi_0`: A vector containing the initial mixture weights.\n",
    "  - $\\pi_0 = \\left[ 0.5, 0.5 \\right]$\n",
    "\n",
    "By initializing these parameters, we can now proceed with the EM algorithm to fit the Gaussian Mixture Model to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bdbbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "pi_0 = np.array([0.5,0.5])\n",
    "mu_0 = np.array([[-1.0,1.0],[1.0,-1.0]])\n",
    "Sigma_0 = np.array([ [[1.0,0.0],[0.0,1.0]] , [[1.0,0.0],[0.0,1.0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034220ce",
   "metadata": {},
   "source": [
    "### Visualizing the Data and Gaussian Components\n",
    "\n",
    "To better understand the Expectation-Maximization (EM) process and the distribution of our data, we will plot both the data points and ellipses representing our Gaussian components. The ellipses will provide a visual interpretation of the covariance and mean of each Gaussian in our mixture model.\n",
    "\n",
    "**plot_cov_ellipse Function**\n",
    "\n",
    "The `plot_cov_ellipse` function plots an ellipse based on a given 2x2 covariance matrix and a location for the ellipse's center. The function also allows the customization of the ellipse's appearance through various parameters.\n",
    "\n",
    "- `cov`: The 2x2 covariance matrix to base the ellipse on.\n",
    "- `pos`: The location of the center of the ellipse.\n",
    "- `volume`: The volume inside the ellipse; default is 0.5.\n",
    "- `ax`: The axis to plot the ellipse on; defaults to the current axis.\n",
    "\n",
    "**plot_components Function**\n",
    "\n",
    "The `plot_components` function uses `plot_cov_ellipse` to plot ellipses for each Gaussian component in our mixture model. It takes in the means (`mu`) and covariances (`Sigma`) of each Gaussian, along with their respective colors, to render these ellipses on the plot.\n",
    "\n",
    "**plot_data Function**\n",
    "\n",
    "The `plot_data` function plots the actual data points. Optionally, it can color the points based on a 'redness' value, allowing us to visually separate data that might belong to different components of the Gaussian mixture.\n",
    "\n",
    "These helper functions will enable us to visualize the Gaussian components and their evolution as we proceed with the EM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742cf873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_cov_ellipse was taken from here:\n",
    "# http://www.nhsilbert.net/source/2014/06/bivariate-normal-ellipse-plotting-in-python/\n",
    "\n",
    "def plot_cov_ellipse(cov, pos, volume=.5, ax=None, fc='none', ec=[0,0,0], a=1, lw=2):\n",
    "    \"\"\"\n",
    "    Plots an ellipse enclosing *volume* based on the specified covariance\n",
    "    matrix (*cov*) and location (*pos*). Additional keyword arguments are passed on to the \n",
    "    ellipse patch artist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        cov : The 2x2 covariance matrix to base the ellipse on\n",
    "        pos : The location of the center of the ellipse. Expects a 2-element\n",
    "            sequence of [x0, y0].\n",
    "        volume : The volume inside the ellipse; defaults to 0.5\n",
    "        ax : The axis that the ellipse will be plotted on. Defaults to the \n",
    "            current axis.\n",
    "    \"\"\"\n",
    "    def eigsorted(cov):\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        return vals[order], vecs[:,order]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "\n",
    "    kwrg = {'facecolor':fc, 'edgecolor':ec, 'alpha':a, 'linewidth':lw}\n",
    "\n",
    "    # Width and height are \"full\" widths, not radius\n",
    "    width, height = 2 * np.sqrt(chi2.ppf(volume,2)) * np.sqrt(vals)\n",
    "    ellip = patches.Ellipse(xy=pos, width=width, height=height, angle=theta, **kwrg)\n",
    "\n",
    "    ax.add_artist(ellip)\n",
    "    \n",
    "\n",
    "def plot_components(mu, Sigma, colours, *args, **kwargs):\n",
    "    '''\n",
    "    Plot ellipses for the bivariate normals with mean mu[:,i] and covariance Sigma[:,:,i]\n",
    "    '''\n",
    "    assert mu.shape[1] == Sigma.shape[2]\n",
    "    assert mu.shape[0] == 2\n",
    "    assert Sigma.shape[0] == 2\n",
    "    assert Sigma.shape[1] == 2\n",
    "    for i in range(mu.shape[1]):\n",
    "        kwargs['ec'] = colours[i]\n",
    "        plot_cov_ellipse(Sigma[i], mu[i], *args, **kwargs)\n",
    "\n",
    "br_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"seismic\",[\"b\",\"r\"])\n",
    "\n",
    "def plot_data(redness=None):\n",
    "    if redness is not None:\n",
    "        assert len(redness) == data.shape[0]\n",
    "        assert all(_ >= 0 and _ <= 1 for _ in redness)\n",
    "        c = redness\n",
    "    else:\n",
    "        c = 'grey'\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(data[:,0],data[:,1], marker='.', s=0.1, alpha=0.3, linewidths=2, c=c, cmap=br_cmap)\n",
    "    plt.xlabel(data_labels[0])\n",
    "    plt.ylabel(data_labels[1])\n",
    "    plt.axis([-2,2,-2,2])\n",
    "    plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b501b0",
   "metadata": {},
   "source": [
    "In the next code cell, we demonstrate how to utilize the plotting functions we've defined earlier. We'll plot the dataset along with the initial guesses for our Gaussian components. This visualization will help us understand our starting point before we begin iterating with the Expectation-Maximization algorithm.\n",
    "\n",
    "The code will only execute if the required variables (`mu_0`, `Sigma_0`, and `data`) have been previously defined. If these variables are in place, the function `plot_data()` will plot the data points, and `plot_components()` will overlay the initial Gaussian ellipses on the same plot.\n",
    "\n",
    "This plot will serve as a reference point, helping us to evaluate how well our Gaussian Mixture Model fits the data as we proceed with the EM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fc871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the required variables are defined\n",
    "if 'mu_0' in locals() and 'Sigma_0' in locals() and 'data' in locals():\n",
    "    # Plot the data points\n",
    "    plot_data()\n",
    "    # Plot the initial Gaussian components\n",
    "    plot_components(mu_0, Sigma_0, ['b', 'r'], 0.2)\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b0b64b",
   "metadata": {},
   "source": [
    "### Optimizing the Gaussian Mixture Model with EM\n",
    "\n",
    "As discussed in lectures, the Gaussian Mixture Model (GMM) can be optimized using the Expectation-Maximization (EM) algorithm. This technique allows for the maximum likelihood estimation of the model parameters $\\mathbf{\\mu}$, $\\mathbf{\\Sigma}$, and $\\mathbf{\\pi}$.\n",
    "\n",
    "**Function Signatures**\n",
    "\n",
    "The primary functions involved in this algorithm are the E-step and the M-step. The suggested function signatures are as follows:\n",
    "\n",
    "- For the E-step: \n",
    "```python\n",
    "def e_step(X, mu, Sigma, pi):\n",
    "```\n",
    "\n",
    "- For the M-step:\n",
    "```python\n",
    "def m_step(X, gamma):\n",
    "```\n",
    "\n",
    "**Helper Functions**\n",
    "\n",
    "In addition to these, a helper function named `weighted_normals` is used to calculate an $ N \\times K $ matrix of weighted normal probabilities, i.e., $ \\pi_{k} \\mathcal{N}( \\mathbf{x}_n \\mid \\mu_k, \\Sigma_k) $.\n",
    "\n",
    "The function signature for the helper function is:\n",
    "\n",
    "```python\n",
    "def weighted_normals(X, mu, Sigma, pi):\n",
    "```\n",
    "\n",
    "**Tips**\n",
    "\n",
    "You can make use of the `scipy.stats` library to compute the multivariate normal distribution probabilities:\n",
    "\n",
    "```python\n",
    "from scipy.stats import multivariate_normal as mvnorm\n",
    "```\n",
    "\n",
    "You can use `mvnorm.pdf(x, mu, sigma)` to compute $ \\mathcal{N}(\\mathbf{x}_N \\mid \\mu_k, \\Sigma_k) $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9155d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_normals(X, mu, Sigma, pi):\n",
    "    \"\"\"\n",
    "    Calculates the numerator of the gamma_i's, i.e., the \n",
    "    weighted normal probabilities for each data point.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K, = pi.shape\n",
    "    w_norms = np.zeros((N,K)) # (N,K)\n",
    "    for k in range(K):\n",
    "        w_norms[:,k] = pi[k] * mvnorm.pdf(data, mu[k], Sigma[k]) # (N,)\n",
    "    return w_norms # (N,K)\n",
    "\n",
    "def e_step(X, mu, Sigma, pi):\n",
    "    \"\"\"\n",
    "    Performs the E-step of the EM algorithm.\n",
    "    \"\"\"\n",
    "    w_norms = weighted_normals(X, mu, Sigma, pi)\n",
    "    gamma = w_norms / np.sum(w_norms, axis=1, keepdims=True)\n",
    "    return gamma\n",
    "\n",
    "def m_step(X, gamma):\n",
    "    \"\"\"\n",
    "    Performs the M-step of the EM algorithm.\n",
    "    \"\"\"\n",
    "    N,D = X.shape\n",
    "    _, K = gamma.shape\n",
    "    Nk = gamma.sum(axis=0) # (N,K)=>(K,)\n",
    "    new_pi = Nk / N # (K,)\n",
    "\n",
    "    # Best, no iteration / sum\n",
    "    new_mu = gamma.T @ X # (K,N)@(N,D) => (K,D)\n",
    "    new_mu /= Nk.reshape(-1,1) # (K,D) / (K,1) => (K,D)\n",
    "\n",
    "    # Best, no iteration / sum\n",
    "    diff = X.reshape(N,1,D) - new_mu.reshape(1,K,D) # (N,1,D) - (1,K,D) => (N,K,D)\n",
    "    scaled_diff = gamma.reshape(N,K,1) * diff # (N,K,1) * (N,K,D) => (N,K,D)\n",
    "    new_Sigma = scaled_diff.transpose((1,2,0)) @ diff.transpose(1,0,2) # (K,D,N) @ (K,N,D) => (K,D,D)\n",
    "    new_Sigma /= Nk.reshape(-1,1,1)  # (K,D,D) / (K,1,1) = (K,D,D)\n",
    "\n",
    "    return new_mu, new_Sigma, new_pi # (K,D), (K,D,D), (K,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005d269b",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "To evaluate the performance of the Gaussian Mixture Model, we will calculate its log-likelihood for given parameters $ \\mathbf{\\mu} $, $ \\mathbf{\\Sigma} $, and $\\mathbf{\\pi}$.\n",
    "\n",
    "The function computes the log-likelihood by first obtaining the weighted normal probabilities using the `weighted_normals` function. Then it sums up the log of these probabilities across all data points. The resulting value will be a single scalar that measures how well the given parameters $ \\mathbf{\\mu} $, $ \\mathbf{\\Sigma} $, and $ \\mathbf{\\pi} $ fit the data $ \\mathbf{X} $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69285a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(X, mu, Sigma, pi):\n",
    "    # Calls the helper function to get the weighted normal probabilities\n",
    "    w = weighted_normals(X, mu, Sigma, pi)\n",
    "    \n",
    "    # Computes the log-likelihood using the obtained probabilities\n",
    "    ll = np.log(w.sum(axis=1)).sum()\n",
    "    \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a7d095",
   "metadata": {},
   "source": [
    "An essential aspect of any iterative algorithm, like the EM algorithm, is to check if it converges. For this purpose, we can plot the log-likelihood of the model against the number of updates made to it.\n",
    "\n",
    "The idea is to run the EM algorithm for a fixed number of trials and calculate the log-likelihood at each step. We will then visualize this information to assess the convergence of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ce8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "mu = mu_0\n",
    "Sigma = Sigma_0\n",
    "pi = pi_0\n",
    "\n",
    "# Number of trials\n",
    "trials = 100\n",
    "\n",
    "# Array to hold the log-likelihoods\n",
    "ll = np.zeros(trials)\n",
    "\n",
    "# Run the EM algorithm\n",
    "for i in range(0, trials):\n",
    "    ll[i] = log_likelihood(data, mu, Sigma, pi)\n",
    "    gamma = e_step(data, mu, Sigma, pi)\n",
    "    (mu, Sigma, pi) = m_step(data, gamma)\n",
    "\n",
    "# Plotting the log-likelihoods from iteration 0\n",
    "plt.title(\"Log-Likelihoods from Iteration 0\")\n",
    "plt.xlabel(\"Number of Updates\")\n",
    "plt.ylabel(\"Log-Likelihood\")\n",
    "plt.plot(ll)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the log-likelihoods starting from iteration 1\n",
    "plt.title(\"Log-Likelihoods from Iteration 1\")\n",
    "plt.xlabel(\"Number of Updates\")\n",
    "plt.ylabel(\"Log-Likelihood\")\n",
    "plt.plot(ll[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c57fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize parameters to some starting values (mu_0, Sigma_0, pi_0)\n",
    "mu = mu_0\n",
    "Sigma = Sigma_0\n",
    "pi = pi_0\n",
    "\n",
    "# Number of iterations for the EM algorithm\n",
    "trials = 50\n",
    "\n",
    "# Array to keep track of the log-likelihood values\n",
    "ll = np.zeros(trials)\n",
    "\n",
    "# Run the EM algorithm\n",
    "for i in range(trials):\n",
    "    # Compute the log-likelihood of the current model\n",
    "    ll[i] = log_likelihood(data, mu, Sigma, pi)\n",
    "    \n",
    "    # Visualize the model every 5 iterations\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Iteration {i}: Current Log-Likelihood = {ll[i]}\")\n",
    "        \n",
    "        # show the color \n",
    "        plot_data(redness = gamma[:,1])\n",
    "\n",
    "        # Plot the Gaussian components\n",
    "        plot_components(mu, Sigma, ['b', 'r'], 0.2)\n",
    "        \n",
    "        # Add a title to indicate the current iteration and log-likelihood\n",
    "        plt.title(f\"Iteration {i}: Log-Likelihood = {ll[i]}\")\n",
    "        plt.show()\n",
    "        \n",
    "    # Perform the E-step to update responsibilities\n",
    "    gamma = e_step(data, mu, Sigma, pi)\n",
    "    \n",
    "    # Perform the M-step to update parameters\n",
    "    mu, Sigma, pi = m_step(data, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe717c",
   "metadata": {},
   "source": [
    "### Off-the-shelf GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a611996f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Set the number of components (start with 2, as expected for thin/thick disk)\n",
    "gmm = GaussianMixture(\n",
    "    n_components=2, \n",
    "    random_state=42, # optional, important for reproducing results\n",
    "    n_init=5, # optional, but a good idea for robustness\n",
    "    means_init=mu_0 # optional, but can be helpful\n",
    ")\n",
    "gmm.fit(data)\n",
    "\n",
    "# Extract the predicted labels and probabilities\n",
    "labels = gmm.predict(data)\n",
    "probabilities = gmm.predict_proba(data)\n",
    "\n",
    "# Plotting the [Fe/H] vs. [Mg/Fe] distribution colored by GMM components\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Left panel: [Fe/H] vs. [Mg/Fe] colored by component labels\n",
    "scatter1 = ax1.scatter(data[:, 0], data[:, 1], c=labels, cmap='cividis', s=5, alpha=0.5)\n",
    "\n",
    "# Because we are now using the off-the-shelf GMM, we have to redefine the ellipse plotting\n",
    "def plot_gmm_ellipses(gmm, ax, colors=['blue', 'red']):\n",
    "    for n, color in enumerate(colors):\n",
    "        mean = gmm.means_[n]\n",
    "        covar = gmm.covariances_[n]\n",
    "        \n",
    "        # Eigenvalues and eigenvectors to draw ellipses\n",
    "        v, w = np.linalg.eigh(covar)\n",
    "        v = np.sqrt(2.0) * np.sqrt(v)  # standard deviations\n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        \n",
    "        angle = np.arctan2(u[1], u[0]) * 180 / np.pi\n",
    "        ellipse = patches.Ellipse(mean, v[0], v[1], 180 + angle, color=color, alpha=0.3)\n",
    "        ax.add_patch(ellipse)\n",
    "\n",
    "plot_gmm_ellipses(gmm, ax1)  # Add ellipses for the GMM components\n",
    "\n",
    "ax1.set_xlabel('[Fe/H]')\n",
    "ax1.set_ylabel('[Mg/Fe]')\n",
    "fig.colorbar(scatter1, ax=ax1, label='GMM Component', orientation='horizontal')\n",
    "\n",
    "# Right panel: [Fe/H] vs. [Mg/Fe] colored by certainty of component assignment\n",
    "certainty = np.max(probabilities, axis=1)  # Take the maximum probability (certainty) for each point\n",
    "scatter2 = ax2.scatter(data[:, 0], data[:, 1], c=certainty, cmap='seismic', s=5, alpha=0.5)\n",
    "ax2.set_xlabel('[Fe/H]')\n",
    "ax2.set_ylabel('[Mg/Fe]')\n",
    "fig.colorbar(scatter2, ax=ax2, label='Component Membership Probability', orientation='horizontal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights of our GMMs\n",
    "gmm.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53784096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means of our GMMs\n",
    "gmm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb232225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariances of our GMMs\n",
    "gmm.covariances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d833bd5",
   "metadata": {},
   "source": [
    "### How many components do we need?\n",
    "\n",
    "We will discuss this in detail on Thursday, but the Bayesian and Aikake Information Criteria tell you how well your model explains the data (with a maximum Likelihood $\\hat{L}$) while applying a penalty for the number of free parameters $k$.\n",
    "\n",
    "For a given model, the AIC is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{AIC} = 2k - 2\\ln\\left(\\hat{L}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "BIC has a similar form but with a different penalty for the number of parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{BIC} = k\\ln(n) - 2\\ln\\left(\\hat{L}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "The lower these values, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8fbe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_components = np.arange(1, 11)\n",
    "bics = []\n",
    "aics = []\n",
    "\n",
    "for n in n_components:\n",
    "    gmm = GaussianMixture(n_components=n, random_state=42, n_init=5)\n",
    "    gmm.fit(data)\n",
    "    bics.append(gmm.bic(data))\n",
    "    aics.append(gmm.aic(data))\n",
    "\n",
    "# Plot BIC/AIC to find the optimal number of components\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(n_components, bics, label='BIC')\n",
    "plt.plot(n_components, aics, label='AIC')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('BIC/AIC')\n",
    "plt.legend()\n",
    "plt.title('BIC and AIC for GMM with Different Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf174a",
   "metadata": {},
   "source": [
    "## Interactive: GALAH DR4 instead of APOGEE DR17\n",
    "\n",
    "Apply what you have learned onto the lower precision GALAH DR4 measurements. Do you get similar results?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "345.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
